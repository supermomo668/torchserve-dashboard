# Inference address and port
inference_address=http://0.0.0.0:8080

# Management address and port
management_address=http://0.0.0.0:8081

# Metrics address and port
metrics_address=http://0.0.0.0:8082

# Number of Netty threads
number_of_netty_threads=32

# Job queue size
job_queue_size=1000

# Model store path (You must specify this)
model_store=/home/model-server/model-store
# Preload model at startup
preload_model=true

# Asynchronous logging
async_logging=true

# number of default workers per model
default_workers_per_model=1

# TorchServe log level
tsa_log_level=DEBUG

# TorchServe log format
ts_log_format="%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %logger{36} - %msg%n"

# TorchServe log file
tsa_log_file=logs/ts_log.log

# TorchServe metrics format
ts_metrics_format="%d{yyyy-MM-dd HH:mm:ss.SSS} org.pytorch.serve.metrics %msg%n"

# TorchServe metrics file
ts_metrics_file=logs/ts_metrics.log

# Install Python dependencies per model
install_py_dep_per_model=true

# Default response timeout in seconds
default_response_timeout=120

# cors_allowed_origin is required to enable CORS, use '*' or your domain name
cors_allowed_origin='*'
# required if you want to use preflight request
cors_allowed_methods=GET, POST, PUT, OPTIONS
# required if the request has an Access-Control-Request-Headers header
cors_allowed_headers=X-Custom-Header

; grpc_inference_port=7070
; grpc_management_port=7071
; number_of_gpu=1
; batch_size=1